{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579ea03b-78f0-41f6-bf6f-f497ff9da0a4",
   "metadata": {},
   "source": [
    "# Lab Assignment Six: Convolutional Network Architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb66b2-183a-4663-baa7-e1548e9ca839",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Selection\n",
    "\n",
    "Ships in Satellite Imagery\n",
    "https://www.kaggle.com/datasets/rhammell/ships-in-satellite-imagery\n",
    "\n",
    "Group: Nick Benso & Benjamin Kuo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a4f8c-c339-4ad8-bb0e-a80f85852ccb",
   "metadata": {},
   "source": [
    "## Preparation (3 points total)  \n",
    "[1.5 points] Choose and explain what metric(s) you will use to evaluate your algorithmâ€™s performance. You should give a detailed argument for why this (these) metric(s) are appropriate on your data. That is, why is the metric appropriate for the task (e.g., in terms of the business case for the task). Please note: rarely is accuracy the best evaluation metric to use. Think deeply about an appropriate measure of performance.\n",
    "\n",
    "[1.5 points] Choose the method you will use for dividi\n",
    "ng your data into training and testing (i.e., are you using Stratified 10-fold cross validation? Shuffle splits? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. Convince me that your cross validation method is a realistic mirroring of how an algorithm would be used in practice. \n",
    "\n",
    "\n",
    "We chose an image dataset comprised of satalite images of ports that include ships in them, the same as used in lab 2. This dataset for identifying ships can be mainly used to monitor port activity to inform supply chain analyses and potentially port management. As a result, the impact of false negatives or positives are not critical to the application. As a result we will be using accuracy to judge the performance as this will give us a good overall performance metric. However, this data may also potentially be used in military intellegence to to identify movements of military assets or recovery operations to find crashed vehicles like planes or ships. In these cases, it would be important to minimize false negatives and use the recall metric, as having a ship slip by in these cases could lead to insufficient military responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89d09b3-2305-40f5-888a-372b9494d3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics as mt\n",
    "from ipywidgets import widgets\n",
    "from skimage import color\n",
    "from skimage import io\n",
    "from imutils import paths\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "tf.random.set_seed(2)\n",
    "np.random.seed(0) # using this to help make results reproducible\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "dir_path = r\"C:\\Users\\nicho\\Downloads\\Garbage classification\\\\\"\n",
    "\n",
    "def loadImg(dir, imgFile):\n",
    "    img = io.imread(dir + imgFile)\n",
    "    return color.rgb2gray(img)\n",
    "\n",
    "def filenameParse(filename):\n",
    "    repl_delim = '__'\n",
    "    str = filename.replace(\".png\", repl_delim)\n",
    "    return str.split('__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6344ea33-e22c-4a3c-ac17-bb4e6fcf6386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d20674c4-1395-4f5b-905e-64b3b708649c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
      "X shape:  (2527, 64, 64, 3)\n",
      "Number of Labels: 6 , Number of Observations: 2527\n",
      "Input Shape:  (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "classes = os.listdir(dir_path)\n",
    "print(classes)\n",
    "\n",
    "target_size = (64, 64)\n",
    "waste_labels = {\"cardboard\":0, \"glass\":1, \"metal\":2, \"paper\":3, \"plastic\":4, \"trash\":5}\n",
    "    \n",
    "def load_dataset(path):\n",
    "  x = []\n",
    "  labels = []\n",
    "  image_paths = sorted(list(paths.list_images(path)))\n",
    "  for image_path in image_paths:\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    x.append(img)\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "    labels.append(waste_labels[label])\n",
    "  x, labels = shuffle(x, labels, random_state=42)\n",
    "  input_shape = (np.array(x[0]).shape[1], np.array(x[0]).shape[1], 3)\n",
    "  print(\"X shape: \", np.array(x).shape)\n",
    "  print(f\"Number of Labels: {len(np.unique(labels))} , Number of Observations: {len(labels)}\")\n",
    "  print(\"Input Shape: \", input_shape)\n",
    "  x = np.asarray(x)\n",
    "  return x, labels, input_shape\n",
    "\n",
    "x, labels, input_shape = load_dataset(dir_path)\n",
    "h, w, _ = input_shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b557bb8a-e2f5-436a-9343-07e79144ea12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Reshape, Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import average \n",
    "from tensorflow.keras.models import  Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = np.array(x)\n",
    "X = X.astype(np.float32)/16.0 - 0.5\n",
    "X = X.reshape((X.shape[0],64,64,3))\n",
    "y = np.array(labels)\n",
    "y = y.astype(np.int32)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    \n",
    "img_wh = 64\n",
    "NUM_CLASSES = 6\n",
    "    \n",
    "y_train_ohe = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_ohe = keras.utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb63e9-5313-47cf-9ea7-1982657b971a",
   "metadata": {},
   "source": [
    "## Modeling (6 points total)\n",
    "[1.5 points]  Setup the training to use data expansion in Keras (also called data augmentation). Explain why the chosen data expansion techniques are appropriate for your dataset. You can use the keras ImageGenerator as a pre-processing step OR in the optimization loop. You can also use the Keras-cv augmenter (a separate package: https://keras.io/keras_cv/Links to an external site.)\n",
    "\n",
    "[2 points] Create a convolutional neural network to use on your data using Keras. Investigate at least two different convolutional network architectures (and investigate changing some parameters of each architecture such as the number of filters--at minimum have two variations of each network for a total of four models trained). Use the method of train/test splitting and evaluation metric that you argued for at the beginning of the lab. Visualize the performance of the training and validation sets per iteration (use the \"history\" parameter of Keras). Be sure that models converge.\n",
    "\n",
    "[1.5 points] Visualize the final results of the CNNs and interpret/compare the performances. Use proper statistics as appropriate, especially for comparing models. \n",
    "\n",
    "[1 points] Compare the performance of your convolutional network to a standard multi-layer perceptron (MLP) using the receiver operating characteristic and area under the curve. Use proper statistical comparison techniques.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b46fec-7faf-4790-abb3-5fef4af2be2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "def summarize_net(net, X_test, y_test, title_text=''):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    yhat = np.argmax(net.predict(X_test), axis=1)\n",
    "    acc = mt.accuracy_score(y_test,yhat)\n",
    "    cm = mt.confusion_matrix(y_test,yhat)\n",
    "    cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', xticklabels=labels,yticklabels=labels)\n",
    "    plt.title(title_text+'{:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036d2fa9-91ce-435c-9fd4-059cf208f3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_mlp_cnn(cnn, mlp, X_test, y_test, labels='auto'):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    if cnn is not None:\n",
    "        yhat_cnn = np.argmax(cnn.predict(X_test), axis=1)\n",
    "        acc_cnn = mt.accuracy_score(y_test,yhat_cnn)\n",
    "        plt.subplot(1,2,1)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_cnn)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm, annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('CNN: '+str(acc_cnn))\n",
    "    \n",
    "    if mlp is not None:\n",
    "        yhat_mlp = np.argmax(mlp.predict(X_test), axis=1)\n",
    "        acc_mlp = mt.accuracy_score(y_test,yhat_mlp)\n",
    "        plt.subplot(1,2,2)\n",
    "        cm = mt.confusion_matrix(y_test,yhat_mlp)\n",
    "        cm = cm/np.sum(cm,axis=1)[:,np.newaxis]\n",
    "        sns.heatmap(cm,annot=True, fmt='.2f',xticklabels=labels,yticklabels=labels)\n",
    "        plt.title('MLP: '+str(acc_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb80544-4612-4064-8527-0a01b8a4c7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "datagen = ImageDataGenerator(\n",
    "     featurewise_center=False,\n",
    "     samplewise_center=False,\n",
    "     featurewise_std_normalization=False,\n",
    "     samplewise_std_normalization=False,\n",
    "     zca_whitening=False,\n",
    "     rotation_range=5, # used, Int. Degree range for random rotations.\n",
    "     width_shift_range=0.1, # used, Float (fraction of total width). Range for random horizontal shifts.\n",
    "     height_shift_range=0.1, # used,  Float (fraction of total height). Range for random vertical shifts.\n",
    "     shear_range=0., # Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
    "     zoom_range=0.,\n",
    "     channel_shift_range=0.,\n",
    "     fill_mode='nearest',\n",
    "     cval=0.,\n",
    "     horizontal_flip=False,\n",
    "     vertical_flip=False,\n",
    "     rescale=None)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23792899-2b2c-4be4-a537-a832cf703895",
   "metadata": {},
   "source": [
    "## CNN Using Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be7075-36be-4b3b-9601-2e8636d89a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "# let's start with an AlexNet style convolutional phase\n",
    "cnn.add(Conv2D(filters=32,\n",
    "                input_shape = (img_wh,img_wh,3),\n",
    "                kernel_size=(3,3), \n",
    "                padding='same', \n",
    "                activation='relu')) # more compact syntax\n",
    "\n",
    "# no max pool before next conv layer!!\n",
    "cnn.add(Conv2D(filters=64,\n",
    "                kernel_size=(3,3), \n",
    "                padding='same', \n",
    "                activation='relu')) # more compact syntax\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "\n",
    "# add one layer on flattened output\n",
    "cnn.add(Dropout(0.25)) # add some dropout for regularization after conv layers\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(128, activation='relu'))\n",
    "cnn.add(Dropout(0.5)) # add some dropout for regularization, again!\n",
    "cnn.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Let's train the model \n",
    "cnn.compile(loss='categorical_crossentropy', # 'categorical_crossentropy' 'mean_squared_error'\n",
    "              optimizer='rmsprop', # 'adadelta' 'rmsprop'\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# the flow method yields batches of images indefinitely, with the given transformations\n",
    "history = cnn.fit(X_train, y_train_ohe, batch_size=32, \n",
    "                   epochs=30, verbose=1,\n",
    "                   validation_data=(X_test,y_test_ohe)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d30b8-442d-46da-891f-4eb0df061ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_net(cnn, X_test, y_test, title_text = 'Using Expansion ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed9332-9a2a-42f1-814c-2bdcccfc1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.title('Training')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Validation')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('Training Loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e09dcb-2c51-4cb4-a45c-ae4659603b5a",
   "metadata": {},
   "source": [
    "## CNN Using ResNet Style Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9ca6b-7179-42c1-81a7-ae58ae6f4fd2",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)\n",
    "You have free reign to provide additional analyses. \n",
    "\n",
    "One idea (required for 7000 level students): Use transfer learning to pre-train the weights of your initial layers of your CNN. Compare the performance when using transfer learning to training without transfer learning (i.e., compare to your best model from above) in terms of classification performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603de18-2291-4540-b4b2-cbf9e89aa9ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
